{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url):\n",
    "    \"\"\"\n",
    "    Utilty function used to get a Beautiful Soup object from a given URL\n",
    "    \"\"\"\n",
    "\n",
    "    session = requests.Session()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}\n",
    "    try:\n",
    "        req = session.get(url, headers=headers)\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "    bs = BeautifulSoup(req.text, 'html.parser')\n",
    "    return bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with different website layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Delivering inclusive urban access: 3 uncomfortable truths\n",
      "URL: https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/\n",
      "\n",
      "\n",
      "The past few decades have been filled with a deep optimism about the role of cities and suburbs across the world. These engines of economic growth host a majority of world population, are major drivers of economic innovation, and have created pathways to opportunities for untold amounts of people.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jeffrey Gutman\n",
      "\n",
      "\t\t\t\t\tFormer Nonresident Fellow, Global Economy and Development\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Adie Tomer\n",
      "\n",
      "\t\t\t\t\tSenior Fellow - Brookings Metro \n",
      "\n",
      " Twitter\n",
      "AdieTomer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "But all is not well within our so-called Urban Century. Rapid urbanization, rising gentrification, concentrated poverty, and shortages of basic infrastructure have combined to create spatial inequity in cities and suburbs across the globe. The challenges of housing, moving, and employing so many people have led to longer travel times, rising housing costs, and unsustainable public spending. Moreover, policymakers are questioning traditional policies and approaches.\n",
      "The past couple years, we’ve led a project at Brookings—Moving to Access—that responds to these spatial challenges by promoting the idea of connecting people to opportunities as a new foundational principle for 21st century urban development. This principle of accessibility is meant to be a corollary to the natural questions we ask ourselves everyday about the communities where we live: Is this the best location to access employment? Are there nearby schools and health services? Is there a market in the neighborhood? How can I get from here to there? Such choices are valid for those with sufficient income. But what about those with more limited resources and thus choices in terms of affordable housing and affordable transport?\n",
      "While economists, planners, and engineers have promoted accessibility for decades, the concept is more often found in textbooks than formal urban policies. In the first stage of this project, we worked with a team of experts to determine what has stalled practical implementation of appropriate policies and practices? “Delivering Inclusive Access,” a report of this initial work, offers a synthesis of what we found and where we believe researchers, policymakers, and practitioners can take this work next. The paper found three central challenges.\n",
      "The fallacy of the single indicator\n",
      "The current transport regime’s approach to measurement is one of outward elegance: The dominant pursuit is speed, and the primary way to measure it is congestion (or what slows us down). Many have come to label this approach a pursuit of “mobility.” It is seen through different, but often singular, measures of how congestion affects a specific roadway. Such singular measures are easily interpreted by policymakers and civil society and can be translated directly into economic analysis of related investments through timesavings. They also conveniently serve such purposes as the internationally agreed-upon Sustainable Development Goals. Yet they actually don’t answer the fundamental question of who can reach where, in how much time, and at what cost.\n",
      "\n",
      "\n",
      "Related Content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Delivering inclusive access\n",
      "\n",
      "Jeffrey Gutman, Adie Tomer, Joseph W. Kane, Nirav Patel, and Ranjitha Shivaram\n",
      "August 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Measuring performance: Accessibility metrics in metropolitan regions around the world\n",
      "\n",
      "Geneviève Boisjoly and Ahmed El-Geneidy\n",
      "August 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Is better access key to inclusive cities?\n",
      "\n",
      "Jeffrey Gutman and Nirav Patel\n",
      "Wednesday, October 5, 2016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accessibility measures can answer those questions, but not through any one measure. First, the variable social, economic, and political contexts related to access mean searching for a single magical indicator is counterintuitive. For example, a wealthy, automobile-centric region like Dallas, Texas, may have very different measurable goals than a denser, poorer region like Dar es Salaam, Tanzania. Second, academic literature is now rife with such complex measures that it could be difficult to communicate their methodology and results with practitioners. The development of a suite of indicators could offer a menu for policymakers and practitioners to judge accessibility based on local objectives, local conditions, and local capacity.\n",
      "The danger of excessive localization\n",
      "Decentralization and empowering local communities is fast becoming a mantra of governance experts across the world, from development practitioners at institutions like the World Bank to city-focused theorists. And for good reason: delegating policy design and fiscal authority directly to the local level helps ensure policies and practices respond to local needs and desires. Yet as urban areas spillover into contiguous and often numerous municipalities, local independence can introduce certain challenges, especially relating to social and environmental externalities. When it comes to transportation and land development, interests of one municipality are often different from its neighbors. And these divergent development goals can exacerbate accessibility challenges within growing regions, spreading people, housing jobs, and other activities further from one another.\n",
      "Addressing spatial inequities in land use and real estate markets require a broader approach to horizontal governance. While there are examples of metropolitan transport authorities, there is less willingness to consider metropolitan or horizontal governance of land use and fiscal policies. For example, should housing be coordinated across an entire region?\n",
      "Countries with a more centralized top down approach to governance, such as France and Germany, have greater ability to formulate metropolitan governance than more decentralized countries such as the U.S. This is not to say there is a one-size-fits-all approach, but there is an opportunity to test different solutions within different governance contexts, comparing how effective each model is to promote spatial inclusivity.\n",
      "The finance community is missing in action\n",
      "Financing is a central topic in infrastructure circles. As maintenance bills from the automobile era come due, populations continue to grow, and fiscal budgets are tight, how can urban areas afford to build enough infrastructure to support future economic growth? In response, new approaches are evolving in fiscal instruments, such as value capture and private-public partnerships. Missing in these discussions, however, are the implications for inclusive access.\n",
      "We conducted a multi-decade review of past academic literature on access and found that there is no clear substantive discussion of accessibility from a fiscal perspective. While urban transport and land use professionals clearly recognize their interrelationship in achieving inclusive accessibility, at least in theory, the fiscal and finance professionals generally ignore the implications of their instruments with regard to inclusivity. The multilateral development banks and their economic evaluations have ignored the distributive impacts until very recently. And the efforts of some countries to incorporate measures through multi-criteria analysis have had limited impact.\n",
      "This gap must be resolved in any effort toward inclusive urban development. There is little doubt that fiscal approaches must carefully assess who ultimately pays and that alternative finance instruments should be adapted to foster access for all.\n",
      "Going forward\n",
      "Our research confirms that there are enormous opportunities to advance accessibility theory into practice. At this point, what is desperately needed is to launch a range of case studies that deal with these issues and challenges under different geographic, governance, and economic contexts. The good news is that many initiatives are already underway, and more robust communication channels and technology can support such efforts. In Chicago, researchers created an online platform to visually explore accessibility by location. In Bogota, researchers evaluated how affordability is a key principle of access. And in Cairo and Kigali, researchers used open tools to achieve new insights for accessibility. Sharing the results of these case studies could lead to a new level of cross-disciplinary approaches to improve accessibility and lessen the effects of spatial inequity.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11696\\3669662544.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrapeNYTimes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Title: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'URL: {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11696\\3669662544.py\u001b[0m in \u001b[0;36mscrapeNYTimes\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mscrapeNYTimes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div.StoryBodyCompanionColumn div p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "\n",
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "\n",
    "def scrapeNYTimes(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find('h1').text\n",
    "    lines = bs.select('div.StoryBodyCompanionColumn div p')\n",
    "    body = '\\n'.join([line.text for line in lines])\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class', 'post-body'}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "\n",
    "url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    "content = scrapeBrookings(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)\n",
    "\n",
    "url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'\n",
    "content = scrapeNYTimes(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    Common base class for all articles/pages\n",
    "    \"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))\n",
    "\n",
    "class Website:\n",
    "    \"\"\" \n",
    "    Contains information about website structure\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        \"\"\"\n",
    "        Utilty function used to get a content string from a Beautiful Soup\n",
    "        object and a selector. Returns an empty string if no object\n",
    "        is found for the given selector\n",
    "        \"\"\"\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def parse(self, site, url):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                content.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'http://oreilly.com', 'h1', 'section#product-description'],\n",
    "    ['Reuters', 'http://reuters.com', 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
    "    ['Brookings', 'http://www.brookings.edu', 'h1', 'div.post-body'],\n",
    "    ['New York Times', 'http://nytimes.com', 'h1', 'div.StoryBodyCompanionColumn div p']\n",
    "]\n",
    "websites = []\n",
    "for row in siteData:\n",
    "    websites.append(Website(row[0], row[1], row[2], row[3]))\n",
    "\n",
    "crawler.parse(websites[0], 'http://shop.oreilly.com/product/0636920028154.do')\n",
    "crawler.parse(\n",
    "    websites[1], 'http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0')\n",
    "crawler.parse(\n",
    "    websites[2],\n",
    "    'https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')\n",
    "crawler.parse(\n",
    "    websites[3], \n",
    "    'https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling through sites with search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "\n",
    "    def __init__(self, topic, url, title, body):\n",
    "        self.topic = topic\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.url = url\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print('New article found for topic: {}'.format(self.topic))\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"Contains information about website structure\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl\n",
    "        self.resultListing = resultListing\n",
    "        self.resultUrl = resultUrl\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        childObj = pageObj.select(selector)\n",
    "        if childObj is not None and len(childObj) > 0:\n",
    "            return childObj[0].get_text()\n",
    "        return ''\n",
    "\n",
    "    def search(self, topic, site):\n",
    "        \"\"\"\n",
    "        Searches a given website for a given topic and records all pages found\n",
    "        \"\"\"\n",
    "        bs = self.getPage(site.searchUrl + topic)\n",
    "        searchResults = bs.select(site.resultListing)\n",
    "        for result in searchResults:\n",
    "            url = result.select(site.resultUrl)[0].attrs['href']\n",
    "            # Check to see whether it's a relative or an absolute URL\n",
    "            if(site.absoluteUrl):\n",
    "                bs = self.getPage(url)\n",
    "            else:\n",
    "                bs = self.getPage(site.url + url)\n",
    "            if bs is None:\n",
    "                print('Something was wrong with that page or URL. Skipping!')\n",
    "                return\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(topic, title, body, url)\n",
    "                content.print()\n",
    "\n",
    "\n",
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'http://oreilly.com', 'https://ssearch.oreilly.com/?q=',\n",
    "        'article.product-result', 'p.title a', True, 'h1', 'section#product-description'],\n",
    "    ['Reuters', 'http://reuters.com', 'http://www.reuters.com/search/news?blob=', 'div.search-result-content',\n",
    "        'h3.search-result-title a', False, 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
    "    ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=',\n",
    "        'div.list-content article', 'h4.title a', True, 'h1', 'div.post-body']\n",
    "]\n",
    "sites = []\n",
    "for row in siteData:\n",
    "    sites.append(Website(row[0], row[1], row[2],\n",
    "                         row[3], row[4], row[5], row[6], row[7]))\n",
    "\n",
    "topics = ['python', 'data science']\n",
    "for topic in topics:\n",
    "    print('GETTING INFO ABOUT: ' + topic)\n",
    "    for targetSite in sites:\n",
    "        crawler.search(topic, targetSite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling Sites through Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "\n",
    "    def __init__(self, name, url, targetPattern, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.targetPattern = targetPattern\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "\n",
    "\n",
    "class Content:\n",
    "\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "        self.visited = []\n",
    "\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def parse(self, url):\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs, self.site.titleTag)\n",
    "            body = self.safeGet(bs, self.site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                content.print()\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Get pages from website home page\n",
    "        \"\"\"\n",
    "        bs = self.getPage(self.site.url)\n",
    "        targetPages = bs.findAll('a', href=re.compile(self.site.targetPattern))\n",
    "        for targetPage in targetPages:\n",
    "            targetPage = targetPage.attrs['href']\n",
    "            if targetPage not in self.visited:\n",
    "                self.visited.append(targetPage)\n",
    "                if not self.site.absoluteUrl:\n",
    "                    targetPage = '{}{}'.format(self.site.url, targetPage)\n",
    "                self.parse(targetPage)\n",
    "\n",
    "\n",
    "reuters = Website('Reuters', 'https://www.reuters.com', '^(/article/)',\n",
    "                  False, 'h1', 'div.StandardArticleBody_body_1gnLA')\n",
    "crawler = Crawler(reuters)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling multiple page types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product(Website):\n",
    "    \"\"\"Contains information for scraping a product page\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, productNumber, price):\n",
    "        Website.__init__(self, name, url, TitleTag)\n",
    "        self.productNumberTag = productNumberTag\n",
    "        self.priceTag = priceTag\n",
    "\n",
    "class Article(Website):\n",
    "    \"\"\"Contains information for scraping an article page\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag, dateTag):\n",
    "        Website.__init__(self, name, url, titleTag)\n",
    "        self.bodyTag = bodyTag\n",
    "        self.dateTag = dateTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parsePage(url):\n",
    "    \n",
    "    if '/ideas/' in url:\n",
    "        \n",
    "\n",
    "oreilly = Website('O\\'Reilly', 'https://oreilly.com', 'h1' '')        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
